{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre process\n",
    "- Get BERT embeddings for sentances and words\n",
    "- Make all sentance Equal length\n",
    "- Make all articles of equal length\n",
    "- Make graph taking sentance as rows and words and label as column\n",
    "- Feed in graph attention model for sentance classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "\n",
    "from IPython.display import clear_output # to clear the large outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../EnglishNews_train.csv\", encoding=\"utf-8\", nrows=10)\n",
    "df = pd.read_csv(\"./newEnglishNews_train.csv\", encoding=\"utf-8\", nrows=1000).dropna().reset_index().drop(['index'], axis=1)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for one article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df[\"Article\"]\n",
    "# articles.head()\n",
    "article = articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary = df[\"Summary\"]\n",
    "# all_summary.head()\n",
    "summary = all_summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clear_output()\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess(text):\n",
    "    text = ' '.join(nltk.word_tokenize(text))\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace the newlines and punctuations with space\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    text = text.translate(str.maketrans(filters, ' '*len(filters)))\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Remove punctuations and numbers\n",
    "    text = ' '.join([word for word in text.split() if word.isalpha()])\n",
    "    # Remove single character\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 2])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(article)\n",
    "# sentences = nltk.sent_tokenize(article) + nltk.sent_tokenize(summary)\n",
    "preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "word_tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in preprocessed_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences)\n",
    "print(word_tokenized_sentences[0])\n",
    "# nltk.word_tokenize(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get total frequency, IDF by (total sentances containing that word)\n",
    "\n",
    "- we can get TF value for each word in each sentance saperatly\n",
    "- Get the IDF value for complete article at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequency = {}\n",
    "total_sentences_containing_word = {}\n",
    "words_idf = {}\n",
    "\n",
    "for sentence in preprocessed_sentences:\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        if word not in words_frequency.keys():\n",
    "            words_frequency[word] = 1\n",
    "        else:\n",
    "            words_frequency[word] += 1\n",
    "    \n",
    "    for word in set(words):\n",
    "        if word not in total_sentences_containing_word.keys():\n",
    "            total_sentences_containing_word[word] = 1\n",
    "        else:\n",
    "            total_sentences_containing_word[word] += 1\n",
    "\n",
    "\n",
    "for word in words_frequency.keys():\n",
    "    words_idf[word] = np.log(len(preprocessed_sentences) / words_frequency[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Word and Sentance Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a BERT model and tokenizer (replace with the specific BERT model you are using)\n",
    "model_name = \"bert-base-uncased\"  # Example: You can use a different pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store sentence and word embeddings\n",
    "sentence_embeddings = []\n",
    "word_embeddings = []\n",
    "\n",
    "# Store the tokenized input IDs, attention masks and token type IDs\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Attention should be done as TF-IDF values are calculated for each word in the sentence\n",
    "\n",
    "\n",
    "# Loop through sentences and tokenize words using NLTK\n",
    "for sentence in preprocessed_sentences:\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Convert words to BERT tokens\n",
    "    tokens = [tokenizer.cls_token] + words + [tokenizer.sep_token]\n",
    "\n",
    "    # Convert tokens to input IDs\n",
    "    _input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Create attention masks by tf-idf values (freq of word in sentence / total sentences containing word)\n",
    "    # attention_mask = [1] * len(_input_ids)\n",
    "    words_tf = {}\n",
    "    for word in words:\n",
    "        if word not in words_tf.keys():\n",
    "            words_tf[word] = 1/words_frequency[word]\n",
    "        else:\n",
    "            words_tf[word] += 1/words_frequency[word]\n",
    "    \n",
    "    attention_mask = [0] + [words_tf[word] * words_idf[word] for word in words] + [0]\n",
    "\n",
    "    # Create an input dictionary in the expected format\n",
    "    input_dict = {\n",
    "        'input_ids': tf.constant([_input_ids]),\n",
    "        'attention_mask': tf.constant([attention_mask]),\n",
    "    }\n",
    "\n",
    "    # Get BERT model output\n",
    "    with tf.device('/GPU:0'):\n",
    "        output = model(input_dict)\n",
    "\n",
    "    # Extract sentence and word embeddings\n",
    "    sentence_embedding = tf.reduce_mean(output.last_hidden_state, axis=1).numpy()  # Sentence embedding\n",
    "    word_embedding = output.last_hidden_state.numpy()  # Word embeddings\n",
    "\n",
    "    # Append to lists\n",
    "    sentence_embeddings.append(sentence_embedding.reshape(768, ))\n",
    "    word_embeddings.append(word_embedding.reshape(-1, 768))\n",
    "\n",
    "    # Append to lists Attention masks and input IDs\n",
    "    input_ids.append(tf.constant([_input_ids]).numpy().reshape(-1))\n",
    "    attention_masks.append(tf.constant([attention_mask]).numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings[0].shape, word_embeddings[0].shape, input_ids[0].shape, attention_masks[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Graph Initializers Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, Activation, Multiply, Concatenate, RepeatVector, Permute, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "def GraphAttentionLayer(inputs, attention_head, activation='relu'):\n",
    "    # Perform linear transformation and add self-attention weights\n",
    "    W = Dense(attention_head, activation=None)(inputs)\n",
    "    attention = Dense(1, activation=None)(W)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "\n",
    "    # Apply attention to the input data\n",
    "    attention = RepeatVector(attention_head)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "    output = Multiply()([inputs, attention])\n",
    "\n",
    "    # Aggregate the output from all attention heads\n",
    "    output = K.sum(output, axis=-2)\n",
    "\n",
    "    output = Activation(activation)(output)\n",
    "    output = Dropout(0.5)(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "def build_gat_model(input_dim, hidden_dim, output_dim, num_heads, num_nodes):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    # Apply Graph Attention Layers\n",
    "    attention_heads = []\n",
    "    for _ in range(num_heads):\n",
    "        attention_head = GraphAttentionLayer(inputs, attention_head=num_heads)\n",
    "        attention_heads.append(attention_head)\n",
    "\n",
    "    # Concatenate the outputs from all attention heads\n",
    "    output_layer = Concatenate()(attention_heads)\n",
    "\n",
    "    # Fully connected layer for final prediction\n",
    "    output_layer = Dense(hidden_dim, activation='relu')(output_layer)\n",
    "    output_layer = Dropout(0.5)(output_layer)\n",
    "\n",
    "    # # Output layer for each node\n",
    "    # output_layers = []\n",
    "    # for _ in range(num_nodes):\n",
    "    #     node_output = Dense(output_dim, activation='softmax')(output_layer)\n",
    "    #     output_layers.append(node_output)\n",
    "\n",
    "\n",
    "    # Output layer with softmax activation for all nodes\n",
    "    output_layer = Dense(num_nodes * output_dim, activation='softmax')(output_layer)\n",
    "\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = max_sentence_len # node features\n",
    "hidden_dim = 64 \n",
    "output_dim = 1 # Define your output dimension (number of classes)\n",
    "num_heads = 4  # Number of attention heads\n",
    "num_nodes = max_article_len # Number of nodes in each graph (number of sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "gat_model = build_gat_model(input_dim, hidden_dim, output_dim, num_heads, num_nodes)\n",
    "\n",
    "# Compile the model\n",
    "gat_model.compile(optimizer=Adam(lr=0.005), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using your training data\n",
    "# gat_model.fit(train_data, [train_labels]*num_nodes, epochs=num_epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new graphs using the trained model\n",
    "# new_graph_predictions = gat_model.predict(new_graph_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
