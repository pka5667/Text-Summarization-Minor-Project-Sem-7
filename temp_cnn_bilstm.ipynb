{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "\n",
    "from IPython.display import clear_output # to clear the large outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../EnglishNews_train.csv\", encoding=\"utf-8\", nrows=10)\n",
    "df = pd.read_csv(\"./newEnglishNews_train.csv\", encoding=\"utf-8\", nrows=1000).dropna().reset_index().drop(['index'], axis=1)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for one article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df[\"Article\"]\n",
    "# articles.head()\n",
    "article = articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary = df[\"Summary\"]\n",
    "# all_summary.head()\n",
    "summary = all_summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clear_output()\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess(text):\n",
    "    text = ' '.join(nltk.word_tokenize(text))\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace the newlines and punctuations with space\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    text = text.translate(str.maketrans(filters, ' '*len(filters)))\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Remove punctuations and numbers\n",
    "    text = ' '.join([word for word in text.split() if word.isalpha()])\n",
    "    # Remove single character\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 2])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(article)\n",
    "# sentences = nltk.sent_tokenize(article) + nltk.sent_tokenize(summary)\n",
    "preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "word_tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in preprocessed_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences)\n",
    "print(word_tokenized_sentences[0])\n",
    "# nltk.word_tokenize(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Word and Sentance Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a BERT model and tokenizer (replace with the specific BERT model you are using)\n",
    "model_name = \"bert-base-uncased\"  # Example: You can use a different pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store sentence and word embeddings\n",
    "sentence_embeddings = []\n",
    "word_embeddings = []\n",
    "\n",
    "# Loop through sentences and tokenize words using NLTK\n",
    "for sentence in preprocessed_sentences:\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Convert words to BERT tokens\n",
    "    tokens = [tokenizer.cls_token] + words + [tokenizer.sep_token]\n",
    "\n",
    "    # Convert tokens to input IDs\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Create an input dictionary in the expected format\n",
    "    input_dict = {\n",
    "        'input_ids': tf.constant([input_ids]),\n",
    "        'attention_mask': tf.constant([[1] * len(input_ids)]),\n",
    "    }\n",
    "\n",
    "    # Get BERT model output\n",
    "    with tf.device('/GPU:0'):\n",
    "        output = model(input_dict)\n",
    "\n",
    "    # Extract sentence and word embeddings\n",
    "    sentence_embedding = tf.reduce_mean(output.last_hidden_state, axis=1).numpy()  # Sentence embedding\n",
    "    word_embedding = output.last_hidden_state.numpy()  # Word embeddings\n",
    "\n",
    "    # Append to lists\n",
    "    sentence_embeddings.append(sentence_embedding)\n",
    "    word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sentance embeddings using CNN and BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(sentences):\n",
    "    max_length = 0\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > max_length:\n",
    "            max_length = len(sentence)\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(word):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal length padding for all sentences\n",
    "max_sentence_length = get_max_length(sentences)\n",
    "for i in range(len(sentences)):\n",
    "    if len(sentences[i]) < max_sentence_length:\n",
    "        sentences[i] = sentences[i] + ['<PAD>'] * (max_sentence_length - len(sentences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Conv1D, MaxPooling1D\n",
    "# sentence_input = Input(shape=(max_sentence_length, embedding_dim)) # Total Words in a sentence * Embedding dimension\n",
    "# cnn_layer = Conv1D(filters=32, kernel_size=3, activation='relu')(sentence_input)\n",
    "# cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "\n",
    "\n",
    "# from keras.layers import Bidirectional, LSTM\n",
    "# lstm_input = Input(shape=(len(sentences), max_sentence_length, 32)) # Total Sentences * Total Words in a sentence * Embedding dimension\n",
    "# lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(lstm_input)\n",
    "\n",
    "# from keras.layers import GlobalMaxPooling1D\n",
    "# pooled_features = GlobalMaxPooling1D()(lstm_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, Bidirectional, LSTM, Dense, RepeatVector\n",
    "from keras.models import Model\n",
    "\n",
    "# Encoder\n",
    "input_article = Input(shape=(max_sentence_length, embedding_dim)) # Total Words in a sentence * Embedding dimension\n",
    "cnn_features = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(input_article)\n",
    "lstm_features = Bidirectional(LSTM(64, return_sequences=True))(cnn_features)\n",
    "\n",
    "# Decoder\n",
    "decoded = LSTM(64, return_sequences=True)(lstm_features)\n",
    "decoded = Dense(embedding_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# Autoencoder Model\n",
    "autoencoder = Model(input_article, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([np.array([get_word_embeddings(word) for word in sentence]) for sentence in sentences])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the autoencoder with articles as both input and target\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(X_train, X_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Model\n",
    "encoder = Model(input_article, [cnn_features, lstm_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_features, contextual_features = encoder.predict(X_train[0].reshape(1, max_sentence_length, embedding_dim))\n",
    "local_features.shape, contextual_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_features = np.reshape(local_features, (max_sentence_length, 32))\n",
    "contextual_features = np.reshape(contextual_features, (max_sentence_length, 128))\n",
    "np.concatenate((local_features, contextual_features), axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding for each sentence in the article\n",
    "def generate_sentence_embedding(sentence): # sentence is a list of words\n",
    "    sentence = [get_word_embeddings(word) for word in sentence]\n",
    "    local_features, contextual_features = encoder.predict(sentence.reshape(1, max_sentence_length, embedding_dim))\n",
    "    local_features = np.reshape(local_features, (max_sentence_length, 32))\n",
    "    contextual_features = np.reshape(contextual_features, (max_sentence_length, 128))\n",
    "    sentence_embedding = np.concatenate((local_features, contextual_features), axis=1)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    sentence_embedding = generate_sentence_embedding(sentence)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a file for embeding values as features for each sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allData.to_csv('./features/embeddings_using_word2vec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
