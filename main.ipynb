{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre process\n",
    "- Make all sentance Equal length\n",
    "- Make all articles of equal length\n",
    "- Get BERT embeddings for sentances and words\n",
    "- Make graph taking sentance as rows and words and label as column\n",
    "- Feed in graph attention model for sentance classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "\n",
    "from IPython.display import clear_output # to clear the large outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../EnglishNews_train.csv\", encoding=\"utf-8\", nrows=10)\n",
    "df = pd.read_csv(\"./newEnglishNews_train.csv\", encoding=\"utf-8\", nrows=10).dropna().reset_index().drop(['index'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and word tokenize all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df[\"Article\"]\n",
    "article = articles[0]\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary = df[\"Summary\"]\n",
    "summary = all_summary[0]\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clear_output()\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess(text):\n",
    "    text = ' '.join(nltk.word_tokenize(text))\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace the newlines and punctuations with space\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    text = text.translate(str.maketrans(filters, ' '*len(filters)))\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    text = ' '.join([word for word in text.split() if word.isalpha()])\n",
    "    \n",
    "    # Remove single character\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 2])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_articles(articles):\n",
    "    preprocessed_articles = []\n",
    "    word_tokenized_articles_list = []\n",
    "    for article in articles:\n",
    "        sentences = nltk.sent_tokenize(article)\n",
    "        preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "        # Word tokenization after preprocessing\n",
    "        word_tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in preprocessed_sentences]\n",
    "        word_tokenized_articles_list.append(word_tokenized_sentences)\n",
    "        preprocessed_articles.append(preprocessed_sentences)\n",
    "    \n",
    "    return word_tokenized_articles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenized_articles = word_tokenize_articles(articles)\n",
    "print(\"Preprocessed and word tokenized article: \", word_tokenized_articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BERT embeddings for setances and words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TF-IDF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IDF from here and make tf for each sentence while making bert embeddings\n",
    "def get_idf(word_tokenized_article):\n",
    "    words_frequency = {}                    # Number of times a word appears in the document\n",
    "    total_sentences_containing_word = {}    # Number of sentences containing a word\n",
    "    words_idf = {}                          # IDF of each word\n",
    "\n",
    "    for sentence in word_tokenized_article:\n",
    "        for word in sentence:\n",
    "            if word not in words_frequency.keys():\n",
    "                words_frequency[word] = 1\n",
    "            else:\n",
    "                words_frequency[word] += 1\n",
    "        \n",
    "        for word in set(sentence):\n",
    "            if word not in total_sentences_containing_word.keys():\n",
    "                total_sentences_containing_word[word] = 1\n",
    "            else:\n",
    "                total_sentences_containing_word[word] += 1\n",
    "\n",
    "\n",
    "    for word in words_frequency.keys():\n",
    "        words_idf[word] = np.log(len(word_tokenized_article) / words_frequency[word])\n",
    "\n",
    "    return words_frequency, words_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sentance and word embedding for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word embeddings for all the words\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a common word embeddings for all the words in all the documents\n",
    "def get_word_embeddings(word_tokenized_articles):\n",
    "    # Get all the words in all the articles\n",
    "    all_words = []\n",
    "    for article in word_tokenized_articles:\n",
    "        for sentence in article:\n",
    "            for word in sentence:\n",
    "                all_words.append(word)\n",
    "    \n",
    "    # Get unique words\n",
    "    unique_words = list(set(all_words))\n",
    "    print(\"Number of unique words: \", len(unique_words))\n",
    "\n",
    "    all_words_embeddings = {}\n",
    "    for word in unique_words:\n",
    "        encoded_input = tokenizer(word, return_tensors='tf')\n",
    "        output = model(encoded_input)\n",
    "        all_words_embeddings[word] = output[0][0][0].numpy()\n",
    "\n",
    "    return all_words_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_word_embeddings = get_word_embeddings(word_tokenized_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a BERT model and tokenizer (replace with the specific BERT model you are using)\n",
    "model_name = \"bert-base-uncased\"  # Example: You can use a different pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word_tokenize_article):\n",
    "    # Initialize lists to store sentence and word embeddings\n",
    "    sentence_embeddings = []\n",
    "    word_embeddings = []\n",
    "\n",
    "    # Store the tokenized input IDs, attention masks and token type IDs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "\n",
    "    for sentence in word_tokenize_article:\n",
    "        # Convert words to BERT tokens\n",
    "        tokens = [tokenizer.cls_token] + sentence + [tokenizer.sep_token]\n",
    "\n",
    "        # Convert tokens to input IDs\n",
    "        _input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Create attention masks by tf-idf values (freq of word in sentence / total sentences containing word)\n",
    "        words_frequency, words_idf = get_idf(word_tokenize_article)\n",
    "\n",
    "        words_tf = {}\n",
    "        for word in sentence:\n",
    "            if word not in words_tf.keys():\n",
    "                words_tf[word] = 1\n",
    "            else:\n",
    "                words_tf[word] += 1\n",
    "        \n",
    "        attention_mask = [0] + [words_tf[word]/words_frequency[word] * words_idf[word] for word in sentence] + [0]\n",
    "\n",
    "\n",
    "        # Create an input dictionary in the expected format\n",
    "        input_dict = {\n",
    "            'input_ids': tf.constant([_input_ids]),\n",
    "            'attention_mask': tf.constant([attention_mask]),\n",
    "        }\n",
    "\n",
    "        # Get BERT model output\n",
    "        with tf.device('/GPU:0'):\n",
    "            output = model(input_dict)\n",
    "\n",
    "        # Extract sentence and word embeddings\n",
    "        sentence_embedding = tf.reduce_mean(output.last_hidden_state, axis=1).numpy()  # Sentence embedding\n",
    "        word_embedding = output.last_hidden_state.numpy()  # Word embeddings\n",
    "\n",
    "        # Append to lists\n",
    "        sentence_embeddings.append(sentence_embedding.reshape(768, ))\n",
    "        # word_embeddings.append(word_embedding.reshape(-1, 768))\n",
    "\n",
    "        # Append to lists Attention masks and input IDs\n",
    "        input_ids.append(tf.constant([_input_ids]).numpy().reshape(-1))\n",
    "        attention_masks.append(tf.constant([attention_mask]).numpy().reshape(-1))\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the sentence and word embeddings for each article \n",
    "all_articles_sentence_embeddings = []\n",
    "for article in word_tokenized_articles:\n",
    "    sentence_embedding = get_embeddings(article)\n",
    "    all_articles_sentence_embeddings.append(sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make all articles of equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_article_len = 0\n",
    "\n",
    "for article in word_tokenized_articles:\n",
    "    max_article_len = max(max_article_len, len(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_articles = []\n",
    "\n",
    "# Padding and truncating the articles\n",
    "for article in word_tokenized_articles:\n",
    "    while len(article) < max_article_len:\n",
    "        article.append([])\n",
    "    while len(article) > max_article_len:\n",
    "        article.pop()\n",
    "    padded_articles.append(article)\n",
    "\n",
    "print(\"Padded and truncated article: \", padded_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also make the sentance embeddings equal to the max_article_len\n",
    "padded_articles_sentence_embeddings = []\n",
    "\n",
    "for article in all_articles_sentence_embeddings:\n",
    "    while len(article) < max_article_len:\n",
    "        article = np.concatenate((article, np.zeros((1, 768))), axis=0)\n",
    "    while len(article) > max_article_len:\n",
    "        article = article[:-1]\n",
    "    padded_articles_sentence_embeddings.append(article)\n",
    "\n",
    "len(padded_articles_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a adjecency matrix for each article\n",
    "\n",
    "\n",
    "For each article need to make a s*w matrix\n",
    "\n",
    "- w:= Unique words in the whole dataset\n",
    "- s:- Number of sentances in each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_words = set()\n",
    "for article in padded_articles:\n",
    "    for sentence in article:\n",
    "        for word in sentence:\n",
    "            all_unique_words.add(word)\n",
    "\n",
    "all_unique_words = list(all_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All unique words size: \", len(all_unique_words))\n",
    "print(\"Length of each articles: \", len(padded_articles[0]))\n",
    "print(\"Matrix size: \", len(padded_articles), len(padded_articles[0]), len(all_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each article create a s*w matrix where s is the number of sentences and w is the number of unique words\n",
    "# and the value of each cell is the tf-idf value of the word in the sentence\n",
    "\n",
    "all_articles_adjacency_matrices = []\n",
    "for article in padded_articles:\n",
    "    article_adjacency_matrix = []\n",
    "    words_frequency, words_idf = get_idf(article)\n",
    "    for sentence in article:\n",
    "        # GET TF-IDF VALUES OF WORDS IN THE SENTENCE\n",
    "        words_tf = {}\n",
    "        for word in sentence:\n",
    "            if word not in words_tf.keys():\n",
    "                words_tf[word] = 1\n",
    "            else:\n",
    "                words_tf[word] += 1\n",
    "        words_tf_idf = {}\n",
    "        for word in words_tf.keys():\n",
    "            words_tf_idf[word] = words_tf[word]/words_frequency[word] * words_idf[word]\n",
    "\n",
    "        # CREATE ADJACENCY MATRIX\n",
    "        sentence_adjacency_matrix = []\n",
    "        for word in all_unique_words:\n",
    "            if word in sentence:\n",
    "                sentence_adjacency_matrix.append(words_tf_idf[word])\n",
    "            else:\n",
    "                sentence_adjacency_matrix.append(0)\n",
    "        article_adjacency_matrix.append(sentence_adjacency_matrix)\n",
    "    all_articles_adjacency_matrices.append(article_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many non zero values are there in the first article\n",
    "count = 0\n",
    "for i in range(len(all_articles_adjacency_matrices[0])):\n",
    "    for j in range(len(all_articles_adjacency_matrices[0][i])):\n",
    "        if all_articles_adjacency_matrices[0][i][j] != 0:\n",
    "            count += 1\n",
    "print(\"Number of non zero values in the first article: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph for each article:-\n",
    "- Node Features\n",
    "- Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentances numbers are from 0 to all the sentances in the article\n",
    "# Words numbers are from #number of sentance to all the unique words in all the articles\n",
    "# So the total number of nodes are the number of sentances + number of unique words\n",
    "\n",
    "# From the adjecency matrix create a list of edges with the weights\n",
    "all_articles_edges = []\n",
    "for article in all_articles_adjacency_matrices:\n",
    "    article_edges = []\n",
    "    for i in range(len(article)):\n",
    "        for j in range(len(article[i])):\n",
    "            if article[i][j] != 0:\n",
    "                article_edges.append((i, j+len(article), article[i][j]))\n",
    "    all_articles_edges.append(article_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_articles_edges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph for first article edges\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot as a bipartite graph with sentances and words\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(all_articles_edges[0])\n",
    "# G.add_weighted_edges_from(all_articles_edges[0][:30])\n",
    "# One side for up to max_article_len nodes and the other side for the rest\n",
    "pos = {}\n",
    "for i in range(len(padded_articles[0])):\n",
    "    pos[i] = (0, i)\n",
    "\n",
    "for i in range(len(padded_articles[0]), len(padded_articles[0])+len(all_unique_words)):\n",
    "    pos[i] = (1, i-len(padded_articles[0]))\n",
    "\n",
    "nx.draw(G, pos, with_labels=False, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto endcoder model\n",
    "\n",
    "- Node Features = [Sentance Embeddings, Word Embeddings]\n",
    "- Edges = [edges between sentace and word node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class GraphAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(GraphAttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[-1], self.output_dim),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        super(GraphAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, adj_matrix):\n",
    "        h = tf.matmul(x, self.kernel)\n",
    "        # print(h.shape)\n",
    "        attn_coef = tf.matmul(tf.matmul(h, adj_matrix), h, transpose_b=True)\n",
    "        attn_coef = tf.nn.leaky_relu(attn_coef, alpha=0.2)\n",
    "        attn_coef = tf.nn.softmax(attn_coef, axis=-1)\n",
    "        output = tf.matmul(attn_coef, h)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "# Create a Keras Graph model with Graph Attention Layer\n",
    "class GraphAutoencoderWithAttention(tf.keras.Model):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GraphAutoencoderWithAttention, self).__init__()\n",
    "\n",
    "        # Graph Attention Layer\n",
    "        self.attention = GraphAttentionLayer(output_dim=hidden_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = layers.Dense(input_size, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, adj_matrix, training=None, mask=None):\n",
    "        x = self.attention(inputs, adj_matrix)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_size = 768  # Size of node features\n",
    "hidden_size = 256\n",
    "model = GraphAutoencoderWithAttention(input_size, hidden_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node features from the sentence embeddings for each article\n",
    "all_articles_node_features = []\n",
    "for article in padded_articles_sentence_embeddings:\n",
    "    article_node_features = []\n",
    "    for sentence in article:\n",
    "        article_node_features.append(sentence)\n",
    "    all_articles_node_features.append(article_node_features)\n",
    "\n",
    "# Convert to tensor node_features_tensor,adjacency_matrix_tensor\n",
    "node_features_tensor = tf.constant(all_articles_node_features)\n",
    "adjacency_matrix_tensor = tf.constant(all_articles_adjacency_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_tensor.shape, adjacency_matrix_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed_features = model(node_features_tensor,adjacency_matrix_tensor)\n",
    "        loss = loss_fn(node_features_tensor, reconstructed_features)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, you can use the learned embeddings for downstream tasks\n",
    "node_features = np.array(all_articles_node_features[0], dtype=np.float32)\n",
    "node_features_tensor = tf.constant(node_features)\n",
    "learned_embeddings = model(node_features_tensor).numpy()\n",
    "# print(learned_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(learned_embeddings), len(learned_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total count greater than 0.1\n",
    "count = 0\n",
    "for i in range(len(learned_embeddings)):\n",
    "    for j in range(len(learned_embeddings[i])):\n",
    "        if learned_embeddings[i][j] > 0.4:\n",
    "            count += 1\n",
    "print(\"Number of values greater than 0.4: \", count, \" out of \", len(learned_embeddings)*len(learned_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dropout, Dense, Layer, LeakyReLU\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# class GraphAttention(Layer):\n",
    "#     def __init__(self, units, attn_heads=1, dropout_rate=0.5, activation='relu', **kwargs):\n",
    "#         super(GraphAttention, self).__init__(**kwargs)\n",
    "#         self.units = units\n",
    "#         self.attn_heads = attn_heads\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(\n",
    "#             shape=(input_shape[0][-1], self.units * self.attn_heads),\n",
    "#             initializer='glorot_uniform',\n",
    "#             trainable=True,\n",
    "#             name='kernel'\n",
    "#         )\n",
    "#         self.b = self.add_weight(\n",
    "#             shape=(self.units * self.attn_heads,),\n",
    "#             initializer='zeros',\n",
    "#             trainable=True,\n",
    "#             name='bias'\n",
    "#         )\n",
    "#         super(GraphAttention, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         X, A = inputs\n",
    "#         h = tf.matmul(X, self.W) + self.b\n",
    "#         h = tf.reshape(h, (-1, self.attn_heads, self.units))\n",
    "#         attn_coeffs = tf.nn.softmax(tf.matmul(tf.matmul(h, tf.transpose(h, [0, 2, 1])), A))\n",
    "#         out = tf.matmul(attn_coeffs, h)\n",
    "#         out = tf.reshape(out, (-1, self.units * self.attn_heads))\n",
    "#         out = self.activation(out)\n",
    "#         return out\n",
    "\n",
    "# def graph_autoencoder_gat(node_features_shape, num_nodes, latent_dim):\n",
    "#     # Encoder\n",
    "#     encoder_inputs = Input(shape=node_features_shape)\n",
    "#     A = Input(shape=(num_nodes, num_nodes))  # Adjacency matrix placeholder\n",
    "\n",
    "#     encoder = GraphAttention(units=64)([encoder_inputs, A])\n",
    "#     encoder = Dropout(0.2)(encoder)\n",
    "#     encoder = GraphAttention(units=latent_dim)([encoder, A])\n",
    "\n",
    "#     # Decoder\n",
    "#     decoder = GraphAttention(units=64)([encoder, A])\n",
    "#     decoder = Dropout(0.2)(decoder)\n",
    "#     decoder_outputs = GraphAttention(units=node_features_shape[0], activation='sigmoid')([decoder, A])\n",
    "\n",
    "#     # Autoencoder model\n",
    "#     autoencoder = Model([encoder_inputs, A], decoder_outputs)\n",
    "\n",
    "#     # Compile the model\n",
    "#     autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#     return autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# # Define a simple graph (you can replace this with your own graph data)\n",
    "# adjacency_matrix = np.array([[[0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 0]],[[0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 0]]], dtype=np.float32)\n",
    "# node_features = np.array([[[1.0,2.0], [2.0,3.0], [3.0,4.0]],[[1.0,2.0], [2.0,3.0], [3.0,4.0]]], dtype=np.float32)\n",
    "\n",
    "# # Create a Keras Graph model\n",
    "# class GraphAutoencoder(tf.keras.Model):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(GraphAutoencoder, self).__init__()\n",
    "\n",
    "#         # Encoder\n",
    "#         self.encoder = layers.Dense(hidden_size, activation='relu')\n",
    "\n",
    "#         # Decoder\n",
    "#         self.decoder = layers.Dense(input_size, activation='sigmoid')\n",
    "\n",
    "#     def call(self, inputs, training=None, mask=None):\n",
    "#         x = self.encoder(inputs)\n",
    "#         x = self.decoder(x)\n",
    "#         return x\n",
    "# # Instantiate the model\n",
    "# input_size = 2 # Size of node features\n",
    "# hidden_size = 64\n",
    "# model = GraphAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# # Convert numpy arrays to TensorFlow tensors\n",
    "# adjacency_matrix_tensor = tf.constant(adjacency_matrix)\n",
    "# node_features_tensor = tf.constant(node_features)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 100\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         reconstructed_features = model(node_features_tensor,adjacency_matrix_tensor)\n",
    "#         loss = loss_fn(node_features_tensor, reconstructed_features)\n",
    "\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy():.4f}')\n",
    "\n",
    "# # After training, you can use the learned embeddings for downstream tasks\n",
    "# node_features = np.array([[2.0,2.0], [3.0,3.0], [4.0,4.0]], dtype=np.float32)\n",
    "# node_features_tensor = tf.constant(node_features)\n",
    "# learned_embeddings = model(node_features_tensor).numpy()\n",
    "# print(learned_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dropout, Reshape\n",
    "# from tensorflow.keras.models import Model\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# class GraphAttention(tf.keras.layers.Layer):\n",
    "#     def __init__(self, units, attn_heads=1, dropout_rate=0.5, activation='relu', **kwargs):\n",
    "#         super(GraphAttention, self).__init__(**kwargs)\n",
    "#         self.units = units\n",
    "#         self.attn_heads = attn_heads\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(\n",
    "#             shape=(input_shape[0][-1], self.units * self.attn_heads),\n",
    "#             initializer='glorot_uniform',\n",
    "#             trainable=True,\n",
    "#             name='kernel'\n",
    "#         )\n",
    "#         self.b = self.add_weight(\n",
    "#             shape=(self.units * self.attn_heads,),\n",
    "#             initializer='zeros',\n",
    "#             trainable=True,\n",
    "#             name='bias'\n",
    "#         )\n",
    "#         super(GraphAttention, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         X, A = inputs\n",
    "#         h = tf.matmul(X, self.W) + self.b\n",
    "#         h = tf.reshape(h, (-1, self.attn_heads, self.units))\n",
    "#         attn_coeffs = tf.nn.softmax(tf.matmul(tf.matmul(h, tf.transpose(h, [0, 2, 1])), A))\n",
    "#         out = tf.matmul(attn_coeffs, h)\n",
    "#         out = tf.reshape(out, (-1, self.units * self.attn_heads))\n",
    "#         out = self.activation(out)\n",
    "#         return out\n",
    "\n",
    "# def graph_autoencoder_gat(node_features_shape, num_nodes, latent_dim):\n",
    "#     # Encoder\n",
    "#     encoder_inputs = Input(shape=node_features_shape)\n",
    "#     A = Input(shape=(num_nodes, num_nodes))  # Adjacency matrix placeholder\n",
    "\n",
    "#     encoder = GraphAttention(units=64)([encoder_inputs, A])\n",
    "#     encoder = Dropout(0.2)(encoder)\n",
    "#     encoder = Reshape((num_nodes * latent_dim,))(encoder)  # Reshape to match decoder input size\n",
    "\n",
    "#     # Decoder\n",
    "#     decoder = Dense(64, activation='relu')(encoder)  # Example dense layer, modify as needed\n",
    "#     decoder = Dropout(0.2)(decoder)\n",
    "#     decoder_outputs = Dense(node_features_shape[0], activation='sigmoid')(decoder)\n",
    "\n",
    "#     # Autoencoder model\n",
    "#     autoencoder = Model([encoder_inputs, A], decoder_outputs)\n",
    "\n",
    "#     # Compile the model\n",
    "#     autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#     return autoencoder\n",
    "\n",
    "# # Generate random sample inputs for training and testing\n",
    "# num_nodes = 50\n",
    "# node_features_shape = (10,)  # Assuming each node has 10 features\n",
    "\n",
    "# # Generate random adjacency matrices (assuming undirected graphs)\n",
    "# adj_matrices_train = [np.random.randint(0, 2, size=(num_nodes, num_nodes)) for _ in range(100)]  # 100 training graphs\n",
    "# adj_matrices_test = [np.random.randint(0, 2, size=(num_nodes, num_nodes)) for _ in range(20)]    # 20 testing graphs\n",
    "\n",
    "# # Generate random node features for each graph\n",
    "# node_features_train = [np.random.rand(num_nodes, node_features_shape[0]) for _ in range(100)]  # Random features for training\n",
    "# node_features_test = [np.random.rand(num_nodes, node_features_shape[0]) for _ in range(20)]    # Random features for testing\n",
    "\n",
    "# # Create the graph autoencoder model\n",
    "# latent_dim = 32  # Adjust as needed\n",
    "# model = graph_autoencoder_gat(node_features_shape, num_nodes, latent_dim)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit([node_features_train, adj_matrices_train], node_features_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Get testing outputs for node classification\n",
    "# # For example, getting the output of the encoder part for node classification\n",
    "# encoded_features = model.layers[2].predict([node_features_test, adj_matrices_test])\n",
    "\n",
    "# # You can use the 'encoded_features' for node classification or further downstream tasks\n",
    "# # Adjust the final layers or outputs for node classification based on your specific task requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
