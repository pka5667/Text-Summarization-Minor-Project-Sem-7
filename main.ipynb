{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre process\n",
    "- Make all sentance Equal length\n",
    "- Make all articles of equal length\n",
    "- Get BERT embeddings for sentances and words\n",
    "- Make graph taking sentance as rows and words and label as column\n",
    "- Feed in graph attention model for sentance classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "\n",
    "from IPython.display import clear_output # to clear the large outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../EnglishNews_train.csv\", encoding=\"utf-8\", nrows=10)\n",
    "df = pd.read_csv(\"./newEnglishNews_train.csv\", encoding=\"utf-8\", nrows=100).dropna().reset_index().drop(['index'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and word tokenize all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df[\"Article\"]\n",
    "article = articles[0]\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary = df[\"Summary\"]\n",
    "summary = all_summary[0]\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clear_output()\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess(text):\n",
    "    text = ' '.join(nltk.word_tokenize(text))\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace the newlines and punctuations with space\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    text = text.translate(str.maketrans(filters, ' '*len(filters)))\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    text = ' '.join([word for word in text.split() if word.isalpha()])\n",
    "    \n",
    "    # Remove single character\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 2])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_articles(articles):\n",
    "    preprocessed_articles = []\n",
    "    word_tokenized_articles_list = []\n",
    "    for article in articles:\n",
    "        sentences = nltk.sent_tokenize(article)\n",
    "        preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "        # Word tokenization after preprocessing\n",
    "        word_tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in preprocessed_sentences]\n",
    "        word_tokenized_articles_list.append(word_tokenized_sentences)\n",
    "        preprocessed_articles.append(preprocessed_sentences)\n",
    "    \n",
    "    return word_tokenized_articles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenized_articles = word_tokenize_articles(articles)\n",
    "print(\"Preprocessed and word tokenized article: \", word_tokenized_articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BERT embeddings for setances and words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TF-IDF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IDF from here and make tf for each sentence while making bert embeddings\n",
    "def get_idf(word_tokenized_article):\n",
    "    words_frequency = {}                    # Number of times a word appears in the document\n",
    "    total_sentences_containing_word = {}    # Number of sentences containing a word\n",
    "    words_idf = {}                          # IDF of each word\n",
    "\n",
    "    for sentence in word_tokenized_article:\n",
    "        for word in sentence:\n",
    "            if word not in words_frequency.keys():\n",
    "                words_frequency[word] = 1\n",
    "            else:\n",
    "                words_frequency[word] += 1\n",
    "        \n",
    "        for word in set(sentence):\n",
    "            if word not in total_sentences_containing_word.keys():\n",
    "                total_sentences_containing_word[word] = 1\n",
    "            else:\n",
    "                total_sentences_containing_word[word] += 1\n",
    "\n",
    "\n",
    "    for word in words_frequency.keys():\n",
    "        words_idf[word] = np.log(len(word_tokenized_article) / words_frequency[word])\n",
    "\n",
    "    return words_frequency, words_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sentance and word embedding for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word embeddings for all the words\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already saved embeddings if available\n",
    "import os\n",
    "if os.path.exists(\"./word_embeddings.npy\"):\n",
    "    all_word_embeddings = np.load(\"./word_embeddings.npy\", allow_pickle=True).item()\n",
    "else:\n",
    "    all_word_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import jit, cuda\n",
    "\n",
    "# Get a common word embeddings for all the words in all the documents\n",
    "# @jit(target_backend='cuda')\n",
    "def get_word_embeddings(word_tokenized_articles):\n",
    "    # Get all the words in all the articles\n",
    "    all_words = []\n",
    "    for article in word_tokenized_articles:\n",
    "        for sentence in article:\n",
    "            for word in sentence:\n",
    "                all_words.append(word)\n",
    "    \n",
    "    # Get unique words\n",
    "    unique_words = list(set(all_words))\n",
    "    print(\"Number of unique words: \", len(unique_words))\n",
    "\n",
    "    embedding = {}\n",
    "    for word in unique_words:\n",
    "        if word in all_word_embeddings.keys():\n",
    "            embedding[word] = all_word_embeddings[word]\n",
    "            continue\n",
    "        encoded_input = tokenizer(word, return_tensors='tf')\n",
    "        output = model(encoded_input)\n",
    "        all_word_embeddings[word] = output[0][0][0].numpy()\n",
    "        embedding[word] = output[0][0][0].numpy()\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_word_embeddings = get_word_embeddings(word_tokenized_articles)\n",
    "# np.save(\"./word_embeddings.npy\", all_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a BERT model and tokenizer (replace with the specific BERT model you are using)\n",
    "model_name = \"bert-base-uncased\"  # Example: You can use a different pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word_tokenize_article):\n",
    "    # Initialize lists to store sentence and word embeddings\n",
    "    sentence_embeddings = []\n",
    "    word_embeddings = []\n",
    "\n",
    "    # Store the tokenized input IDs, attention masks and token type IDs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "\n",
    "    for sentence in word_tokenize_article:\n",
    "        # Convert words to BERT tokens\n",
    "        tokens = [tokenizer.cls_token] + sentence + [tokenizer.sep_token]\n",
    "\n",
    "        # Convert tokens to input IDs\n",
    "        _input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Create attention masks by tf-idf values (freq of word in sentence / total sentences containing word)\n",
    "        words_frequency, words_idf = get_idf(word_tokenize_article)\n",
    "\n",
    "        words_tf = {}\n",
    "        for word in sentence:\n",
    "            if word not in words_tf.keys():\n",
    "                words_tf[word] = 1\n",
    "            else:\n",
    "                words_tf[word] += 1\n",
    "        \n",
    "        attention_mask = [0] + [words_tf[word]/words_frequency[word] * words_idf[word] for word in sentence] + [0]\n",
    "\n",
    "\n",
    "        # Create an input dictionary in the expected format\n",
    "        input_dict = {\n",
    "            'input_ids': tf.constant([_input_ids]),\n",
    "            'attention_mask': tf.constant([attention_mask]),\n",
    "        }\n",
    "\n",
    "        # Get BERT model output\n",
    "        with tf.device('/GPU:0'):\n",
    "            output = model(input_dict)\n",
    "\n",
    "        # Extract sentence and word embeddings\n",
    "        sentence_embedding = tf.reduce_mean(output.last_hidden_state, axis=1).numpy()  # Sentence embedding\n",
    "        word_embedding = output.last_hidden_state.numpy()  # Word embeddings\n",
    "\n",
    "        # Append to lists\n",
    "        sentence_embeddings.append(sentence_embedding.reshape(768, ))\n",
    "        # word_embeddings.append(word_embedding.reshape(-1, 768))\n",
    "\n",
    "        # Append to lists Attention masks and input IDs\n",
    "        input_ids.append(tf.constant([_input_ids]).numpy().reshape(-1))\n",
    "        attention_masks.append(tf.constant([attention_mask]).numpy().reshape(-1))\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the sentence and word embeddings for each article \n",
    "all_articles_sentence_embeddings = []\n",
    "for article in word_tokenized_articles:\n",
    "    sentence_embedding = get_embeddings(article)\n",
    "    all_articles_sentence_embeddings.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make all articles of equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_article_len = 0\n",
    "\n",
    "for article in word_tokenized_articles:\n",
    "    max_article_len = max(max_article_len, len(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_articles = []\n",
    "\n",
    "# Padding and truncating the articles\n",
    "for article in word_tokenized_articles:\n",
    "    while len(article) < max_article_len:\n",
    "        article.append([])\n",
    "    while len(article) > max_article_len:\n",
    "        article.pop()\n",
    "    padded_articles.append(article)\n",
    "\n",
    "print(\"Padded and truncated article: \", padded_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also make the sentance embeddings equal to the max_article_len\n",
    "padded_articles_sentence_embeddings = []\n",
    "\n",
    "for article in all_articles_sentence_embeddings:\n",
    "    while len(article) < max_article_len:\n",
    "        article = np.concatenate((article, np.zeros((1, 768))), axis=0)\n",
    "    while len(article) > max_article_len:\n",
    "        article = article[:-1]\n",
    "    padded_articles_sentence_embeddings.append(article)\n",
    "\n",
    "len(padded_articles_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a adjecency matrix for each article\n",
    "\n",
    "\n",
    "For each article need to make a s*w matrix\n",
    "\n",
    "- w:= Unique words in the whole dataset\n",
    "- s:- Number of sentances in each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_words = set()\n",
    "for article in padded_articles:\n",
    "    for sentence in article:\n",
    "        for word in sentence:\n",
    "            all_unique_words.add(word)\n",
    "\n",
    "all_unique_words = list(all_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All unique words size: \", len(all_unique_words))\n",
    "print(\"Length of each articles: \", len(padded_articles[0]))\n",
    "print(\"Matrix size: \", len(padded_articles), len(padded_articles[0]), len(all_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each article create a (s+w)*(s+w) matrix where s is the number of sentences and w is the number of unique words\n",
    "# and the value of each cell is the tf-idf value of the word in the sentence, sentence in the sentence and word in the word are zero\n",
    "\n",
    "all_articles_adjacency_matrices = []\n",
    "for article in padded_articles:\n",
    "    article_adjacency_matrix = []\n",
    "    words_frequency, words_idf = get_idf(article)\n",
    "    for sentence in article:\n",
    "        # GET TF-IDF VALUES OF WORDS IN THE SENTENCE\n",
    "        words_tf = {}\n",
    "        for word in sentence:\n",
    "            if word not in words_tf.keys():\n",
    "                words_tf[word] = 1\n",
    "            else:\n",
    "                words_tf[word] += 1\n",
    "        words_tf_idf = {}\n",
    "        for word in words_tf.keys():\n",
    "            words_tf_idf[word] = words_tf[word]/words_frequency[word] * words_idf[word]\n",
    "\n",
    "        # CREATE ADJACENCY MATRIX\n",
    "        sentence_adjacency_matrix = []\n",
    "        for _ in range(len(article)):\n",
    "            sentence_adjacency_matrix.append(0)\n",
    "        \n",
    "        for word in all_unique_words:\n",
    "            if word in sentence:\n",
    "                sentence_adjacency_matrix.append(words_tf_idf[word])\n",
    "            else:\n",
    "                sentence_adjacency_matrix.append(0)\n",
    "        article_adjacency_matrix.append(sentence_adjacency_matrix)\n",
    "\n",
    "    for _ in range(len(all_unique_words)):\n",
    "        article_adjacency_matrix.append([0]*(len(article)+len(all_unique_words)))\n",
    "    all_articles_adjacency_matrices.append(article_adjacency_matrix)\n",
    "\n",
    "print(\"Adjacency matrix size: \", len(all_articles_adjacency_matrices), len(all_articles_adjacency_matrices[0]), len(all_articles_adjacency_matrices[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many non zero values are there in the first article\n",
    "count = 0\n",
    "for i in range(len(all_articles_adjacency_matrices[0])):\n",
    "    for j in range(len(all_articles_adjacency_matrices[0][i])):\n",
    "        if all_articles_adjacency_matrices[0][i][j] != 0:\n",
    "            count += 1\n",
    "print(\"Number of non zero values in the first article: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph for each article:-\n",
    "- Node Features\n",
    "- Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentances numbers are from 0 to all the sentances in the article\n",
    "# Words numbers are from #number of sentance to all the unique words in all the articles\n",
    "# So the total number of nodes are the number of sentances + number of unique words\n",
    "\n",
    "# From the adjecency matrix create a list of edges with the weights\n",
    "all_articles_edges = []\n",
    "for article in all_articles_adjacency_matrices:\n",
    "    article_edges = []\n",
    "    for i in range(len(article)):\n",
    "        for j in range(len(article[i])):\n",
    "            if article[i][j] != 0:\n",
    "                article_edges.append((i, j, article[i][j]))\n",
    "    all_articles_edges.append(article_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_articles_edges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph for first article edges\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot as a bipartite graph with sentances and words\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(all_articles_edges[0])\n",
    "# G.add_weighted_edges_from(all_articles_edges[0][:30])\n",
    "# One side for up to max_article_len nodes and the other side for the rest\n",
    "pos = {}\n",
    "for i in range(len(padded_articles[0])):\n",
    "    pos[i] = (0, i)\n",
    "\n",
    "for i in range(len(padded_articles[0]), len(all_articles_adjacency_matrices[0])):\n",
    "    pos[i] = (1, i)\n",
    "\n",
    "nx.draw(G, pos, with_labels=False, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto endcoder model\n",
    "\n",
    "- Node Features = [Sentance Embeddings, Word Embeddings]\n",
    "- Edges = [edges between sentace and word node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class GraphAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(GraphAttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[-1], self.output_dim),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        super(GraphAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, adj_matrix):\n",
    "        h = tf.matmul(x, self.kernel)\n",
    "        # print(h.shape)\n",
    "        attn_coef = tf.matmul(tf.matmul(h, adj_matrix), h, transpose_b=True)\n",
    "        attn_coef = tf.nn.leaky_relu(attn_coef, alpha=0.2)\n",
    "        attn_coef = tf.nn.softmax(attn_coef, axis=-1)\n",
    "        output = tf.matmul(attn_coef, h)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "# Create a Keras Graph model with Graph Attention Layer\n",
    "class GraphAutoencoderWithAttention(tf.keras.Model):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GraphAutoencoderWithAttention, self).__init__()\n",
    "\n",
    "        # Graph Attention Layer\n",
    "        self.attention = GraphAttentionLayer(output_dim=hidden_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = layers.Dense(input_size, activation='sigmoid')\n",
    "        # self.decoder = layers.Dense(1, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, adj_matrix, training=None, mask=None):\n",
    "        x = self.attention(inputs, adj_matrix)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_size = 768  # Size of node features\n",
    "hidden_size = max_article_len + len(all_unique_words)  # Hidden dimension size for attention layer\n",
    "model = GraphAutoencoderWithAttention(input_size, hidden_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node features from the sentence embeddings for each article\n",
    "all_articles_node_features = []\n",
    "for article in padded_articles_sentence_embeddings:\n",
    "    article_node_features = []\n",
    "    for sentence in article:\n",
    "        article_node_features.append(sentence)\n",
    "    for word in all_unique_words:\n",
    "        article_node_features.append(all_articles_word_embeddings[word])\n",
    "    all_articles_node_features.append(article_node_features)\n",
    "\n",
    "# Convert to tensor node_features_tensor,adjacency_matrix_tensor\n",
    "# node_features_tensor = tf.constant(all_articles_node_features)\n",
    "# adjacency_matrix_tensor = tf.constant(all_articles_adjacency_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train test split node features tensor and adjacency matrix tensor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# node_features_train, node_features_test, adjacency_matrix_train, adjacency_matrix_test = train_test_split(all_articles_node_features, all_articles_adjacency_matrices, test_size=0.2, random_state=42)\n",
    "# len(adjacency_matrix_train), len(adjacency_matrix_train[0]), len(adjacency_matrix_train[0][1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split node features tensor and adjacency matrix tensor first by index and then by value\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(range(len(all_articles_node_features)), range(len(all_articles_adjacency_matrices)), test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_train = []\n",
    "node_features_test = []\n",
    "adjacency_matrix_train = []\n",
    "adjacency_matrix_test = []\n",
    "\n",
    "for i in x_train:\n",
    "    node_features_train.append(all_articles_node_features[i])\n",
    "\n",
    "for i in y_train:\n",
    "    adjacency_matrix_train.append(all_articles_adjacency_matrices[i])\n",
    "\n",
    "for i in x_test:\n",
    "    node_features_test.append(all_articles_node_features[i])\n",
    "\n",
    "for i in y_test:\n",
    "    adjacency_matrix_test.append(all_articles_adjacency_matrices[i])\n",
    "\n",
    "len(adjacency_matrix_train), len(adjacency_matrix_train[0]), len(adjacency_matrix_train[0][1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor node_features_tensor,adjacency_matrix_tensor\n",
    "node_features_train_tensor = tf.constant(node_features_train)\n",
    "node_features_test_tensor = tf.constant(node_features_test)\n",
    "\n",
    "adjacency_matrix_train_tensor = tf.constant(adjacency_matrix_train)\n",
    "adjacency_matrix_test_tensor = tf.constant(adjacency_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_train_tensor.shape, adjacency_matrix_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # For each node features and adjacency matrix pair\n",
    "    for node_features, adjacency_matrix in zip(node_features_train_tensor, adjacency_matrix_train_tensor):\n",
    "        # print(node_features_tensor.shape, adjacency_matrix_tensor.shape)\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed_features = model(node_features,adjacency_matrix)\n",
    "            loss = loss_fn(node_features, reconstructed_features)\n",
    "\n",
    "    # update the weights after each epoch for the whole dataset\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate new embeddings for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, you can use the learned embeddings for downstream tasks\n",
    "node_features = np.array(all_articles_node_features[0], dtype=np.float32)\n",
    "adjacency_matrix = np.array(all_articles_adjacency_matrices[0], dtype=np.float32)\n",
    "node_features_tensor = tf.constant(node_features)\n",
    "adjacency_matrix_tensor = tf.constant(adjacency_matrix)\n",
    "learned_embeddings = model(node_features_tensor, adjacency_matrix_tensor).numpy()\n",
    "# print(learned_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles_predicted_embeddings = []\n",
    "for node_features, adjacency_matrix in zip(node_features_test_tensor, adjacency_matrix_test_tensor):\n",
    "    reconstructed_features = model(node_features,adjacency_matrix)\n",
    "    test_articles_predicted_embeddings.append(reconstructed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_articles_predicted_embeddings), test_articles_predicted_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentance_predecited_embeddings = [article[:max_article_len] for article in test_articles_predicted_embeddings]\n",
    "test_sentance_predecited_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_articles_predicted_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentance Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in test_articles_predicted_embeddings[0]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def page_rank(article_predicted_embeddings):\n",
    "    '''\n",
    "    Apply page rank on these embeddings of dimesion (max_article_len, 768)\n",
    "    Get the top 15% sentences\n",
    "    Return the summary\n",
    "    '''\n",
    "    \n",
    "    # Calculate cosine similarity matrix\n",
    "    cosine_similarity_matrix = cosine_similarity(article_predicted_embeddings)\n",
    "    # print(cosine_similarity_matrix)\n",
    "    # distances = 1 - cosine_similarity_matrix\n",
    "    distances = np.round(cosine_similarity_matrix.clip(min=0), 2)\n",
    "    # print(distances)\n",
    "    # print(distances.shape)\n",
    "\n",
    "    # Adjecency list calucate\n",
    "    m = [[] for _ in range(len(distances))]\n",
    "    for i in range(len(distances)):\n",
    "        for j in range(len(distances[i])):\n",
    "            if distances[i][j] >= 0.95:\n",
    "                m[i].append(j)\n",
    "\n",
    "    # Page rank calculation using custom furmula\n",
    "    n = len(m)\n",
    "    d, it = .85, 1000\n",
    "    rank = [1 for _ in range(n)]\n",
    "\n",
    "    # Calculate rank for each iteration\n",
    "    for _ in range(it):\n",
    "        for i in range(n):\n",
    "            rank[i] = (1-d) + d * sum([rank[x]/len(m[x]) for x in m[i]])\n",
    "    \n",
    "    # print(rank)\n",
    "\n",
    "    # Get the top 15% sentences with the highest rank and return them as a list of sentences index\n",
    "    top_15_percent = int(max_article_len * 0.15)\n",
    "    top_15_percent_sentences = []\n",
    "    for i in range(top_15_percent):\n",
    "        top_15_percent_sentences.append(rank.index(max(rank)))\n",
    "        rank[rank.index(max(rank))] = 0\n",
    "\n",
    "    return top_15_percent_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(test_articles_predicted_embeddings[0].numpy())\n",
    "a.extend(list(test_articles_predicted_embeddings[1].numpy()))\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_rank(test_articles_predicted_embeddings[0][:max_article_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ranked_index_for_each_test_article = []\n",
    "\n",
    "for article in test_articles_predicted_embeddings:\n",
    "    page_ranked_index_for_each_test_article.append(page_rank(article[:max_article_len]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_new_summary = []\n",
    "\n",
    "for i in range(len(page_ranked_index_for_each_test_article)):\n",
    "    article_number = x_test[i]\n",
    "    sentence_tokaniized_article = nltk.sent_tokenize(articles[article_number])\n",
    "    summary = \"\"\n",
    "    for index in page_ranked_index_for_each_test_article[i]:\n",
    "        if index < len(sentence_tokaniized_article):\n",
    "            summary += sentence_tokaniized_article[index] + \" \"\n",
    "    all_articles_new_summary.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "def similarity_using_rouge(sentance1, sentance2):\n",
    "    rouge_scores = rouge.get_scores(sentance1, sentance2)[0]\n",
    "    rouge_1 = rouge_scores['rouge-1']\n",
    "    rouge_2 = rouge_scores['rouge-2']\n",
    "    rouge_l = rouge_scores['rouge-l']\n",
    "    return rouge_1, rouge_2, rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_df = pd.DataFrame(columns=['rouge-1 r', 'rouge-1 p', 'rouge-1 f', 'rouge-2 r', 'rouge-2 p', 'rouge-2 f', 'rouge-l r', 'rouge-l p', 'rouge-l f'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1_r, rouge_1_p, rouge_1_f = 0, 0, 0\n",
    "rouge_2_r, rouge_2_p, rouge_2_f = 0, 0, 0\n",
    "rouge_l_r, rouge_l_p, rouge_l_f = 0, 0, 0\n",
    "\n",
    "\n",
    "for i in range(len(all_articles_new_summary)):\n",
    "    rouge_1, rouge_2, rouge_l = similarity_using_rouge(all_articles_new_summary[i], all_summary[y_test[i]])\n",
    "    rouge_1_r += rouge_1['r'] / len(y_test)\n",
    "    rouge_1_p += rouge_1['p'] / len(y_test)\n",
    "    rouge_1_f += rouge_1['f'] / len(y_test)\n",
    "    rouge_2_r += rouge_2['r'] / len(y_test)\n",
    "    rouge_2_p += rouge_2['p'] / len(y_test)\n",
    "    rouge_2_f += rouge_2['f'] / len(y_test)\n",
    "    rouge_l_r += rouge_l['r'] / len(y_test)\n",
    "    rouge_l_p += rouge_l['p'] / len(y_test)\n",
    "    rouge_l_f += rouge_l['f'] / len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rouge scores: \")\n",
    "print(\"Rouge-1: \")\n",
    "print(f\"r: {rouge_1_r}\\np: {rouge_1_p}\\nf: {rouge_1_f}\\n\")\n",
    "print(\"Rouge-2: \")\n",
    "print(f\"r: {rouge_2_r}\\np: {rouge_2_p}\\nf: {rouge_2_f}\\n\")\n",
    "print(\"Rouge-l: \")\n",
    "print(f\"r: {rouge_l_r}\\np: {rouge_l_p}\\nf: {rouge_l_f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rouge_df = rouge_df.append({'rouge-1 r': rouge_1_r, 'rouge-1 p': rouge_1_p, 'rouge-1 f': rouge_1_f, 'rouge-2 r': rouge_2_r, 'rouge-2 p': rouge_2_p, 'rouge-2 f': rouge_2_f, 'rouge-l r': rouge_l_r, 'rouge-l p': rouge_l_p, 'rouge-l f': rouge_l_f}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_df.to_csv(\"./rouge_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
