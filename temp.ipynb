{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(GraphAttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[-1], self.output_dim),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        super(GraphAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, adj_matrix):\n",
    "        h = tf.matmul(x, self.kernel)\n",
    "        # print(h.shape)\n",
    "        attn_coef = tf.matmul(tf.matmul(h, adj_matrix), h, transpose_b=True)\n",
    "        attn_coef = tf.nn.leaky_relu(attn_coef, alpha=0.2)\n",
    "        attn_coef = tf.nn.softmax(attn_coef, axis=-1)\n",
    "        output = tf.matmul(attn_coef, h)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "# Create a Keras Graph model with Graph Attention Layer\n",
    "class GraphAutoencoderWithAttention(tf.keras.Model):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GraphAutoencoderWithAttention, self).__init__()\n",
    "\n",
    "        # Graph Attention Layer\n",
    "        self.attention = GraphAttentionLayer(output_dim=hidden_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = layers.Dense(input_size, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, adj_matrix, training=None, mask=None):\n",
    "        x = self.attention(inputs, adj_matrix)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple graph (you can replace this with your own graph data)\n",
    "adjacency_matrix = np.array([[0, 1, 0, 1, 2], [1, 0, 1, 2, 3],  [0, 1, 0, 3, 4], [1, 0, 1, 2, 5], [1, 0, 1, 2, 5]], dtype=np.float32)\n",
    "node_features = np.array([[1.0,2.0], [2.0,1.0], [3.0,3.0], [4.0,2.0], [5.0,1.0], [1,2], [3,1]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the model\n",
    "input_size = 2  # Size of node features\n",
    "hidden_size = 5\n",
    "model = GraphAutoencoderWithAttention(input_size, hidden_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4.2461\n",
      "Epoch 11/100, Loss: 3.1633\n",
      "Epoch 21/100, Loss: 2.9898\n",
      "Epoch 31/100, Loss: 2.9544\n",
      "Epoch 41/100, Loss: 2.9439\n",
      "Epoch 51/100, Loss: 2.9397\n",
      "Epoch 61/100, Loss: 2.9374\n",
      "Epoch 71/100, Loss: 2.9360\n",
      "Epoch 81/100, Loss: 2.9350\n",
      "Epoch 91/100, Loss: 2.9342\n",
      "[[0.9981042  0.9971571 ]\n",
      " [0.9982247  0.99729365]\n",
      " [0.998242   0.9973133 ]\n",
      " [0.9982425  0.9973138 ]\n",
      " [0.9982425  0.9973139 ]\n",
      " [0.9981042  0.9971571 ]\n",
      " [0.99824107 0.9973123 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert numpy arrays to TensorFlow tensors\n",
    "adjacency_matrix_tensor = tf.constant(adjacency_matrix)\n",
    "node_features_tensor = tf.constant(node_features)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed_features = model(node_features_tensor,adjacency_matrix_tensor)\n",
    "        loss = loss_fn(node_features_tensor, reconstructed_features)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy():.4f}')\n",
    "\n",
    "# After training, you can use the learned embeddings for downstream tasks\n",
    "learned_embeddings = model(node_features_tensor, adjacency_matrix_tensor).numpy()\n",
    "print(learned_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
